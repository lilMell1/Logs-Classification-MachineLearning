import json
import os
from datetime import datetime
from sklearn.metrics import precision_score, recall_score, f1_score
from config.settings import LOG_STORAGE_FILE


def evaluate_metrics(current_run_logs, last_result_learning, summary_file_path):
    if not current_run_logs or not isinstance(current_run_logs, list):
        print("âš  No valid logs received for evaluation.")
        return None

    y_true_current = [log["realAnswer"].lower() for log in current_run_logs]
    y_pred_current = [log["predicted_category"].lower() for log in current_run_logs]

    correct = sum(1 for a, b in zip(y_true_current, y_pred_current) if a == b)
    total = len(current_run_logs)

    confidence_values = []
    for log in current_run_logs:
        try:
            confidence_values.append(float(log["confidence"]))
        except (ValueError, TypeError):
            print(f"âš  Skipping invalid confidence value: {log.get('confidence')}")

    avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0

    # Read historical logs
    if not os.path.exists(LOG_STORAGE_FILE):
        print("âš  No historical log file found.")
        return None

    try:
        with open(LOG_STORAGE_FILE, "r") as f:
            all_logs = json.load(f)
    except Exception as e:
        print(f"âŒ Failed to load historical logs: {e}")
        return None

    if not all_logs or not isinstance(all_logs, list):
        print("âš  Historical logs file is empty or invalid.")
        return None

    y_true_all = [log["realAnswer"].lower() for log in all_logs]
    y_pred_all = [log["predicted_category"].lower() for log in all_logs]

    precision = precision_score(y_true_all, y_pred_all, average="weighted", zero_division=0)
    recall = recall_score(y_true_all, y_pred_all, average="weighted", zero_division=0)
    f1 = f1_score(y_true_all, y_pred_all, average="weighted", zero_division=0)

    print(f"\nğŸ“ˆ Evaluation Metrics on ALL historical runs:")
    print(f"ğŸ“Š Precision: {precision:.3f}")
    print(f"ğŸ“Š Recall:    {recall:.3f}")
    print(f"ğŸ“Š F1 Score:  {f1:.3f}")

    run_summary = {
        "timestamp": datetime.now().isoformat(),
        "total_logs": total,
        "correct_predictions": correct,
        "accuracy": round(correct / total, 4),
        "average_confidence": round(avg_confidence, 4),
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1_score": round(f1, 4)
    }

    # âœ… 1. ×©××•×¨ ×¨×§ ××ª ×”×¡×©×Ÿ ×”× ×•×›×—×™ ×‘×§×•×‘×¥ ×©×œ last_result_learning
    with open(last_result_learning, "w") as f:
        json.dump(run_summary, f, indent=4)

    # âœ… 2. ×•×¢×“×›×Ÿ ××ª ×”×§×•×‘×¥ ×©×œ summary_results.json ×›×¨×©×™××” (append)
    if os.path.exists(summary_file_path):
        try:
            with open(summary_file_path, "r") as f:
                all_runs = json.load(f)
                if not isinstance(all_runs, list):
                    all_runs = []
        except:
            all_runs = []
    else:
        all_runs = []

    all_runs.append(run_summary)

    with open(summary_file_path, "w") as f:
        json.dump(all_runs, f, indent=4)

    print("\nâœ… Evaluation results saved:")
    print(f" - Current run â†’ {last_result_learning}")
    print(f" - Summary of all runs â†’ {summary_file_path}")

    return run_summary
